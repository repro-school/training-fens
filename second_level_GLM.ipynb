{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/repro-school/training-fens/blob/main/second_level_GLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2fCqNRqrWHO"
      },
      "source": [
        "# Higher-level analyses.\n",
        "\n",
        "In this notebook, we will be essentially following on from the [previous notebook](https://colab.research.google.com/drive/1dMgMQQddOPPCs7sBOnH9YNFQk7SsWtb-?usp=sharing) on first-level analyses we did a few weeks ago. Here, we will be expanding in several ways:\n",
        "\n",
        "1. We will be performing higher-level GLM analyses.\n",
        "2. We will take a look at the issue of *correcting for multiple statistical tests*.\n",
        "3. We will take a look at *region of interest* analyses.\n",
        "\n",
        "\n",
        "## Want something to listen to while you work? ðŸŽ§\n",
        "\n",
        "[Here](https://www.youtube.com/watch?v=jfKfPfyJRdk) is a playlist I listened to while preparing this notebook. It never lets me down.\n",
        "\n",
        "\n",
        "## The data\n",
        "We will be using the same movie-watching data as in the previous notebook. \n",
        "However, this time, we will be using a larger quantity of data:\n",
        "\n",
        "Specifically:\n",
        "\n",
        "1. Instead of data from 1 subject, we will be modeling data from **5 subjects**.\n",
        "2. From each subject there are **two runs of data**. In other words, they were scanned twice. In this case, they watched exactly the same movie each time they were scanned. \n",
        "\n",
        "Our goal is to generate a set of group-level results that allow us to generalise our results to the population, as is illustrated below.\n",
        "\n",
        "<img src = 'https://docs.google.com/drawings/d/e/2PACX-1vQxCH3WU3nTqFlHUZb49rf9zioivGQ-flVfRpwmXQx7OF5Wm_1T6gFMYQqpqt-NPITNHUaRoVYEREgT/pub?w=965&h=745' width = \"600\" height = \"\" >\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTVt78h4s6WE"
      },
      "source": [
        "## Getting started\n",
        "\n",
        "\n",
        "First however, in order to install the software that renders our visualisations and all the relevant packages, we need to run the same, rather time-consuming cell that we ran at the start of the last notebook ðŸ˜´ .\n",
        "\n",
        "Run the below cell and make yourself a cup of tea/ coffee while you wait. Again, you will see a green tick when it is all finished. This cell shouldnt take more than a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrGZyJOJrN-4"
      },
      "outputs": [],
      "source": [
        "#@title â†“ --- Run this cell by pressing the play button below\n",
        "# this cell installs some dependencies. \n",
        "# feel free to disregard the output this generates\n",
        "\n",
        "!apt -qq install inkscape &> /dev/null\n",
        "!pip -qq install nibabel nilearn &> /dev/null\n",
        "!pip -qq install git+https://github.com/gallantlab/pycortex.git#egg=pycortex &> /dev/null\n",
        "! pip install googledrivedownloader &> /dev/null\n",
        "! pip install mne\n",
        "\n",
        "from platform import python_version\n",
        "pversion='.'.join(python_version().split('.') [:-1])\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import nibabel as nb\n",
        "import nilearn as nl\n",
        "from nilearn.surface import load_surf_data\n",
        "import os, shutil, urllib.request\n",
        "\n",
        "from matplotlib import rc\n",
        "from matplotlib import cm\n",
        "import matplotlib as mpl\n",
        "rc('animation', html='jshtml')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline \n",
        "\n",
        "import pickle\n",
        "\n",
        "#\n",
        "# this cell ensures that we can work with our own surface from within the colab environment\n",
        "#\n",
        "wrong_filestore_location='/usr/share/pycortex/db'\n",
        "\n",
        "os.makedirs('/content/pycortex/db', exist_ok=True)\n",
        "os.makedirs('/content/pycortex/colormaps', exist_ok=True)\n",
        "os.makedirs('/content/data', exist_ok=True)\n",
        "\n",
        "with open('/usr/local/lib/python{pversion}/dist-packages/cortex/defaults.cfg'.format(pversion=pversion), 'r') as f:\n",
        "  file_source = f.read()\n",
        "\n",
        "replace_string = file_source.replace(wrong_filestore_location, '/content/pycortex/db') #save output \n",
        "with open('/usr/local/lib/python{pversion}/dist-packages/cortex/defaults.cfg'.format(pversion=pversion), 'w') as f:\n",
        "  f.write(replace_string)   \n",
        "\n",
        "os.chdir('/tmp/')\n",
        "!git clone https://github.com/gallantlab/pycortex.git\n",
        "!cp /tmp/pycortex/filestore/colormaps/* /content/pycortex/colormaps/\n",
        "\n",
        "#\n",
        "# and we'll download our average hcp subject for pycortex visualization\n",
        "#\n",
        "\n",
        "pycortex_sj_URL = \"https://ndownloader.figshare.com/files/25768841\"\n",
        "\n",
        "urllib.request.urlretrieve(pycortex_sj_URL, os.path.join('/content/pycortex/db', 'hcp_999999.zip'))\n",
        "!unzip -qq /content/pycortex/db/hcp_999999.zip -d /content/pycortex/db/\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print('Everything all set up!')\n",
        "\n",
        "!pip install --upgrade --no-cache-dir gdown\n",
        "\n",
        "import gdown\n",
        "url3 = 'https://drive.google.com/drive/folders/1_LwMr0mphubfVupdYUBk800YoM8yuqTY?usp=sharing&confirm=t'\n",
        "output3 = '/content/data/HCP-MMP'\n",
        "gdown.download_folder(url=url3, output=output3, quiet=False)\n",
        "\n",
        "\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "\n",
        "class MMP_masker:\n",
        "    def __init__(self,MMPloc):\n",
        "        self.MMPloc=MMPloc\n",
        "        self.load()\n",
        "        \n",
        "    def load(self):    \n",
        "        self.annotfile_L = os.path.join(self.MMPloc,'lh.HCP-MMP1.annot')\n",
        "        self.annotfile_R = os.path.join(self.MMPloc,'rh.HCP-MMP1.annot')\n",
        "        \n",
        "        self.lh_labels, self.lh_ctab, self.lh_names = nib.freesurfer.io.read_annot(self.annotfile_L)\n",
        "        self.rh_labels, self.rh_ctab, self.rh_names = nib.freesurfer.io.read_annot(self.annotfile_R)\n",
        "        self.lh_names=self.decode_list(self.lh_names)\n",
        "        self.rh_names=self.decode_list(self.rh_names)\n",
        "        \n",
        "    def decode_list(self,inlist):\n",
        "        outlist=[x.decode() for x in inlist]\n",
        "        return outlist\n",
        "        \n",
        "    def get_roi_index(self,label,hem='L'):\n",
        "        \n",
        "        if hem=='L':\n",
        "            n2search=self.lh_names\n",
        "        elif hem=='R':\n",
        "            n2search=self.rh_names\n",
        "            \n",
        "        idx=n2search.index('{hem}_{label}_ROI'.format(label=label,hem=hem))\n",
        "        \n",
        "        return idx\n",
        "    \n",
        "    def get_roi_verts(self,label):\n",
        "        Lverts,Rverts=np.where(self.lh_labels==self.get_roi_index(label))[0],np.where(self.rh_labels==self.get_roi_index(label,hem='R'))[0]\n",
        "        return Lverts, Rverts\n",
        "    \n",
        "    def downsample(self,inarray,vertsperhem=10242):\n",
        "        \n",
        "        outarray=inarray[:vertsperhem]\n",
        "        return outarray\n",
        "        \n",
        "    def make_roi_mask(self,label,downsample=False,boolean=True):\n",
        "        L_empty,R_empty=np.zeros(len(self.lh_labels)),np.zeros(len(self.rh_labels))\n",
        "        Lverts,Rverts=self.get_roi_verts(label)\n",
        "        L_empty[Lverts]=1\n",
        "        R_empty[Rverts]=1\n",
        "        \n",
        "        if downsample==True:\n",
        "            L_empty,R_empty=self.downsample(L_empty),self.downsample(R_empty)\n",
        "        \n",
        "        combined_mask=np.concatenate([L_empty,R_empty])\n",
        "        \n",
        "        if boolean==True:\n",
        "             L_empty,R_empty,combined_mask=L_empty.astype(bool),R_empty.astype(bool),combined_mask.astype(bool)\n",
        "            \n",
        "        return L_empty, R_empty, combined_mask\n",
        "    \n",
        "    def make_composite_mask(self,labels):\n",
        "        roimasks=np.sum([self.make_roi_mask(label) for label in labels],axis=0)        \n",
        "        return roimasks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGukWu70uJYs"
      },
      "source": [
        "## Inspecting the data\n",
        "\n",
        "Now let's download and inspect the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhwnKNndKcRL"
      },
      "outputs": [],
      "source": [
        "url1 = 'https://drive.google.com/uc?id=1_KiLfTnxEKr_Geh6r0fIGzQgKZfYGqSY&confirm=t'\n",
        "output1 = '/content/data/run1_data.npz'\n",
        "gdown.download(url1, output1, quiet=False)\n",
        "\n",
        "url2 = 'https://drive.google.com/uc?id=1_Lg_HkdLdW1CWxEGXQOjO-UHaHXMUHmo&confirm=t'\n",
        "output2 = '/content/data/run2_data.npz'\n",
        "gdown.download(url2, output2, quiet=False)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "ts_data1=np.load('/content/data/run1_data.npz',allow_pickle=True)\n",
        "ts_data2=np.load('/content/data/run2_data.npz',allow_pickle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2taPnMWBvweV"
      },
      "outputs": [],
      "source": [
        "run_1_data=np.array(ts_data1['arr_0'])\n",
        "run_2_data=np.array(ts_data2['arr_0'])\n",
        "run_1_data.shape, run_2_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J52PT2K94Pfp"
      },
      "outputs": [],
      "source": [
        "run_1_data.size + run_2_data.size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6m8wbX9vyzD"
      },
      "source": [
        "Yikes, thats a lot of data ðŸ˜¨. 243097200 data points in total !!\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "Let's break this down:\n",
        "\n",
        "\n",
        "Our data conist of two arrays of size (5, 118584, 205)\n",
        "\n",
        "As we know from the last notebook the *118584* refers to the number of voxels and *205* refers to the number of timepoints.\n",
        "\n",
        "We have *2 arrays* because there are two functional runs for each participant.\n",
        "\n",
        "The *5*, then, corresponds to the number of participants in our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EU7bTBo5QvZ"
      },
      "source": [
        "As in the previous notebook, lets plot the time-averaged response onto the brain. This time, lets do it for each subject. We will plot the first run only to avoid cluttering the figure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6pdxHGnxk49"
      },
      "outputs": [],
      "source": [
        "import cortex\n",
        "subject_means=np.mean(run_1_data,axis=2)\n",
        "f, s = plt.subplots(2,3,figsize=(24,10))\n",
        "f.suptitle('Subject mean responses for run 1')\n",
        "\n",
        "\n",
        "for c,v in enumerate(s.reshape(-1)):\n",
        "  if c==5:\n",
        "    break\n",
        "  vas_rgb_v=cortex.Vertex(subject_means[c], subject='hcp_999999', cmap='cubehelix',vmin=np.nanmin(subject_means[c]),vmax=np.nanmax(subject_means[c])) \n",
        "  v.title.set_text(\"Subject {sub}\".format(sub=str(c+1)))\n",
        "\n",
        "  cortex.quickshow(vas_rgb_v, with_labels=False, with_rois=False,fig=v);\n",
        "\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1No4roa82vg"
      },
      "source": [
        "Here, although the general profile is the same (large response in visual  and auditory cortices), we can actually see quite a bit of individual subject variation in the mean BOLD response to the movie. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6GSRI9b7vDd"
      },
      "source": [
        "To illustrate the individual variability in responses, here I take the mean data within V1 and plot it for each of the 5 subjects, which appear as seperate lines. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugFcU2vc7I5p"
      },
      "outputs": [],
      "source": [
        "V1_data=np.mean(run_1_data[:,cortex.utils.get_roi_verts('hcp_999999','V1')['V1']],axis=1)\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "figure(figsize=(12, 3), dpi=80)\n",
        "plt.title('Mean V1 reponses for each subject in run 1')\n",
        "\n",
        "\n",
        "plt.plot(V1_data.T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFSZOlpj86-d"
      },
      "source": [
        "Here we see that even though subjects are watching exactly the same movie, the responses in visual cortex are quite fundamentally different from one another. \n",
        "\n",
        "Let's see what the between-subject correlations are for mean V1 timecourses for each participant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfPnV7kg8xme"
      },
      "outputs": [],
      "source": [
        "np.corrcoef(V1_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvBTFwxY9O0t"
      },
      "source": [
        "Wow, some of the correlations are as low as .03. There are a number of reasons why we might see such divergent brain responses.\n",
        "\n",
        "<img src = 'https://dragonflytraining.files.wordpress.com/2013/10/man-with-question-01.png?w=600&h=600' width = \"100\" height = \"\" >\n",
        "\n",
        "**QUESTION:** What might be some of the sources of this between-subject variability in responses?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipz7FV3YFA28"
      },
      "source": [
        "Now lets look at the mean response in A1 (primary auditory cortex) for each subject"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqHvISch90_c"
      },
      "outputs": [],
      "source": [
        "A1_data=np.mean(run_1_data[:,cortex.utils.get_roi_verts('hcp_999999','A1')['A1']],axis=1)\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "figure(figsize=(12, 3), dpi=80)\n",
        "\n",
        "plt.plot(A1_data.T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55antqf_FQcZ"
      },
      "source": [
        "\n",
        "\n",
        "<img src = 'https://dragonflytraining.files.wordpress.com/2013/10/man-with-question-01.png?w=600&h=600' width = \"100\" height = \"\" >\n",
        "\n",
        "\n",
        "\n",
        "**QUESTION:** In both V1 and A1 we can see that all subjects have a large, elevated response at the very start of the movie. What do you think might be going on here?\n",
        "\n",
        "\n",
        "# Take a break 1.\n",
        "\n",
        "Ok, lets take a break and discuss what we have done so far.\n",
        "\n",
        "<img src = 'https://upload.wikimedia.org/wikipedia/commons/8/81/Stop_sign.png' width = \"200\" height = \"\" >\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfmW2h6TK-rt"
      },
      "source": [
        "# Modeling our data\n",
        "\n",
        "## Combining across runs - a fixed effects model.\n",
        "\n",
        "Ok, next we want to try and model our individual subject data. To make this simple, let's only interest ourselves in one effect, which is a simple contrast between two ${\\beta}$ (or  $c\\hat{\\beta}$).\n",
        "\n",
        " Let's say we are interested in finding regions that have a signficantly larger response to bodies than to scenes.\n",
        "\n",
        " In other words, testing the null hypothesis:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "\n",
        "${\\beta}$  (bodies) - ${\\beta}$  (scenes) ==0\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "First of all, let's make our design matrix (X in the GLM equation). This is no different from our last notebook - so if you want to review the stages that we went through to generate that - then take a look at the other notebook.\n",
        "\n",
        "For now, I have hidden the code in the below cell, since it is rather a lot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pJss2R2H1_ks"
      },
      "outputs": [],
      "source": [
        "#@title â†“ --- Run this cell by pressing the play button below\n",
        "# this cell installs some dependencies. \n",
        "# feel free to disregard the output this generates\n",
        "body_starts=[26, 49, 61, 72, 88, 93, 98, 104, 116, 139, 156, 164, 170]\n",
        "body_ends=[33, 57, 71, 75, 90, 95, 99, 111, 136, 140, 158, 166, 190]\n",
        "text_starts=[0, 21, 27, 74, 83, 91, 107, 150, 137]\n",
        "text_ends=[5, 25, 31, 78, 89, 95, 109, 152, 140]\n",
        "scene_starts=[20, 34, 79, 112, 146, 153, 159, 166]\n",
        "scene_ends=[21, 36, 82, 116, 150, 156, 162, 170]\n",
        "\n",
        "\n",
        "def make_boxcar_from_times(secsbeg,secsend,full_length=205):\n",
        "\n",
        "    event=np.zeros(full_length)      \n",
        "\n",
        "    for c,v in enumerate(secsbeg):\n",
        "        event[secsbeg[c]:secsend[c]]=1 \n",
        "        \n",
        "    return event\n",
        "\n",
        "body_event=make_boxcar_from_times(body_starts,body_ends)\n",
        "scene_event=make_boxcar_from_times(scene_starts,scene_ends)\n",
        "text_event=make_boxcar_from_times(text_starts,text_ends)\n",
        "events=np.vstack([body_event,scene_event,text_event])\n",
        "\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "\n",
        "from scipy import signal\n",
        "from nilearn.glm.first_level.hemodynamic_models import spm_hrf, spm_time_derivative, spm_dispersion_derivative\n",
        "\n",
        "def create_hrf(hrf_params=[1.0, 1.0, 0.0],TR=1):\n",
        "    \"\"\"\n",
        "        \n",
        "    construct single or multiple HRFs        \n",
        "    Parameters\n",
        "    ----------\n",
        "    hrf_params : TYPE, optional\n",
        "    DESCRIPTION. The default is [1.0, 1.0, 0.0].\n",
        "    Returns\n",
        "    -------\n",
        "    hrf : ndarray\n",
        "    the hrf.\n",
        "    \n",
        "    \n",
        "    Adapted from prfpy\n",
        "    \"\"\"\n",
        "        \n",
        "    hrf = np.array([np.ones_like(hrf_params[1])*hrf_params[0] *spm_hrf(tr=TR,oversampling=1,time_length=40)[...,np.newaxis],\n",
        "    hrf_params[1] *spm_time_derivative(tr=TR,oversampling=1,time_length=40)[...,np.newaxis],hrf_params[2] *\n",
        "    spm_dispersion_derivative(tr=TR,oversampling=1,time_length=40)[...,np.newaxis]]).sum(axis=0)\n",
        "    \n",
        "\n",
        "    return hrf.T\n",
        "\n",
        "hrf=create_hrf()\n",
        "\n",
        "\n",
        "def convolve(hrf,regressor):\n",
        "    \n",
        "    \"\"\"\n",
        "    Performs standard HRF convolution with a regressor.     \n",
        "    Parameters\n",
        "    ----------\n",
        "    hrf : a HRF (i.e. created by create_hrf)\n",
        "    Regressor: A fmri timecourse regressor\n",
        "    Returns\n",
        "    -------\n",
        "    conv : The convolve regressor.\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    \n",
        "    hrf_shape = np.ones(len(regressor.shape), dtype=np.int)\n",
        "    hrf_shape[-1] = hrf.shape[-1]\n",
        "    conv=signal.fftconvolve(regressor,hrf.reshape(hrf_shape), mode='full', axes=(-1))[..., :regressor.shape[-1]]\n",
        "    return conv\n",
        "\n",
        "regs=convolve(hrf,events)\n",
        "import pandas as pd\n",
        "nldm=pd.DataFrame(regs.T,columns=['body','scene','text'])\n",
        "nldm=nldm.assign(intercept=1)\n",
        "\n",
        "X=nldm.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXCa4bE6MM6b"
      },
      "source": [
        "Ok, now we have generated our design matrix. Remember, the columns are\n",
        "\n",
        "1. bodies\n",
        "2. scenes\n",
        "3. text\n",
        "4. and the intercept."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27jgOocM24qV"
      },
      "outputs": [],
      "source": [
        "plt.matshow(X, fignum=1, aspect='auto')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B808pxN2Mk2y"
      },
      "source": [
        "Take another look at the figure below. The first thing that we need to do is grapple with the fact that we have two runs per subject. We can use a *fixed-effects model* (FFX) to get a combined estimate of our effect across runs. This allows us to generate one effect per subject that we can then pass up to our group analysis. \n",
        "\n",
        "&nbsp;\n",
        "\n",
        "\n",
        "<img src = 'https://docs.google.com/drawings/d/e/2PACX-1vQxCH3WU3nTqFlHUZb49rf9zioivGQ-flVfRpwmXQx7OF5Wm_1T6gFMYQqpqt-NPITNHUaRoVYEREgT/pub?w=965&h=745' width = \"600\" height = \"\" >\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "As I mentioned in the lecture, this is essentially akin to concatenating the Y (data) and X (design matrix) for each run into one 'long' X and Y.\n",
        "\n",
        "In other words, we model the data as if this is all emanated from one long run - and we essentially completely ignore the fact that there are two different runs in our modeling. Let's concatenate the data from each run into one long dataset with 410 TRs instead of 205.\n",
        "\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQPVxTBrNPl5"
      },
      "outputs": [],
      "source": [
        "full_data=np.concatenate([run_1_data,run_2_data],axis=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqLzYukaPFg7"
      },
      "outputs": [],
      "source": [
        "full_data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeUaEop9P-vB"
      },
      "source": [
        "Thus, we now have a 410 TR timeseries for each of the 5 subjects.\n",
        "\n",
        "For instance, here is what our new mean V1 data looks like. I have drawn a blue line that illustrates where one run ended and the other began. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQ-fptxfPCCR"
      },
      "outputs": [],
      "source": [
        "figure(figsize=(15, 8), dpi=80)\n",
        "V1_data_long=np.mean(full_data[:,cortex.utils.get_roi_verts('hcp_999999','V1')['V1']],axis=1)\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "figure(figsize=(12, 3), dpi=80)\n",
        "plt.title('Mean V1 reponses for each subject concatenated across runs')\n",
        "plt.text(x=100, y=1.2,s='Run 1')\n",
        "plt.text(x=300, y=1.2,s= 'Run 2')\n",
        "\n",
        "plt.plot(V1_data_long.T)\n",
        "plt.axvline(x=205,linewidth=4, color='b')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6TvIheeO8Du"
      },
      "source": [
        "Note that now we have concatenated our data from each run (*Y*) into one timeseries of 410 TRs, in order to fit the model, we will also need to do the same for *X* -  or our design matrix.\n",
        "\n",
        "Since the subjects watched the same movie in each run, the timing of events will be no different. Therefore, this basically just involves duplicating our original matrix and stacking the two design matrices on top of each other. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdKPzZJWNbMh"
      },
      "outputs": [],
      "source": [
        "X_long=np.tile(X.T,2).T\n",
        "plt.matshow(X_long, fignum=1, aspect='auto')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZzXRcmO3xIM"
      },
      "source": [
        "Now all that remains is to perform a GLM on this concatenated data for each subject. This will give us one contrast image per subject - thereby effectively combing our data across runs.\n",
        "\n",
        "Since we already went through all the nuts and bolts of GLM modeling in our previous notebook it is now trivial for us to go through each one of these subjects, solve the GLM equation for each one and compute the body>scene contrast for each subject.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "Below I define a function that takes the design matrix (X), the data (Y) and a contrast vector to compute:\n",
        "1. the numerator of the t statistic ($c\\hat{\\beta}$) (effect or cope in FSL terminology).\n",
        "2. The denominator of the t statistic (uncertainty, or varcope in FSL terminology).\n",
        "3. and the t statistic itself (effect/ uncertainty) for a given contrast.\n",
        "\n",
        "If you read the code below, youll notice this is all **exactly** the same things we did in the last notebook for our single subject analyses. The only difference is that now we are now doing this for each subject in our dataset.\n",
        "\n",
        "I have annotated the code below to refresh your memory of what we are doing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SOBSEXv4dy9"
      },
      "outputs": [],
      "source": [
        "import numpy.linalg as npl\n",
        "\n",
        "def compute_contrast(X,Y,contrast=[1,-1,0,0]): # Our contrast is bodies v scenes (this essentially subtracts the scene column from the body column)\n",
        "  contrast1=np.array(contrast)\n",
        "  Y=np.nan_to_num(Y.T)\n",
        "\n",
        "\n",
        "  # GLM\n",
        "  beta=np.linalg.solve(X.T @ X, X.T @ Y) # Solve the GLM equation, estimating beta.\n",
        "\n",
        "  # Contrast/effect/cope (numerator of t statistic)\n",
        "  t_numerator=np.dot(contrast1,beta) # The contrast image is the dot product of the contrast vector and estimated betas.\n",
        "  \n",
        "  # Uncertainty/ varcope stuff\n",
        "  n = Y.shape[0] # Timepoints\n",
        "  df_error = n - npl.matrix_rank(X) # Timepoints- precictors.\n",
        "  print(df_error)\n",
        "  yhat=np.dot(X,beta) # Model predictions.\n",
        "  E = Y - yhat # Error\n",
        "  sigma_2 = np.sum(E ** 2, axis=0) / df_error # Sum of squared errors\n",
        "\n",
        "  design_variance= contrast1.dot(npl.pinv(X.T.dot(X))).dot(contrast1) # Design variance\n",
        "  t_denominator=np.sqrt(sigma_2 * design_variance) # T denominator\n",
        "\n",
        "  # Compute t as the ratio of effect and uncertainty.\n",
        "  tstat=t_numerator/t_denominator\n",
        "\n",
        "\n",
        "  return t_numerator,t_denominator,tstat,sigma_2 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IQ4WbjtRIeJ"
      },
      "source": [
        "Ok, lets do all the fitting and contrast estimation for each subject in a *loop*.\n",
        "\n",
        " In other words, below we apply the function above to all 5 subjects in our full dataset and return the results in a list.\n",
        "\n",
        "Hence, we will get a list, with each of the 5 entries in the list containing the results for each subject."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuZ-wSEB6XLx"
      },
      "outputs": [],
      "source": [
        "results_fixed_effects=[compute_contrast(X_long,v) for c,v in enumerate(full_data)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfkyNyqBReBH"
      },
      "source": [
        "Great, now we can plot the contrast ($c\\hat{\\beta}$) images for ${\\beta}$  (bodies) - ${\\beta}$  (scenes) that we estimated for each subject. Here, brighter regions indicate a larger difference in the response to ${\\beta}$  (bodies) - ${\\beta}$ (scenes) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8N_5d4p5OIB"
      },
      "outputs": [],
      "source": [
        "f, s = plt.subplots(2,3,figsize=(24,10))\n",
        "f.suptitle(r'$\\beta$'+ ' body -' r' $\\beta$'+ ' house' +'contrast image for each subject estimated by fixed-effects modeling across runs')\n",
        "\n",
        "\n",
        "for c,v in enumerate(s.reshape(-1)):\n",
        "  if c==5:\n",
        "    break\n",
        "  betahat=results_fixed_effects[c][0]\n",
        "  vas_rgb_v=cortex.Vertex(betahat, subject='hcp_999999', cmap='cubehelix',vmin=np.nanmin(betahat),vmax=np.nanmax(betahat)) \n",
        "  v.title.set_text(\"Subject {sub}\".format(sub=str(c+1)))\n",
        "\n",
        "  cortex.quickshow(vas_rgb_v, with_labels=False, with_rois=False,fig=v);\n",
        "\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-ovWmX2T9I3"
      },
      "source": [
        "Cool. Keep in mind that each subject also has the numerator for their t statistic (or VARCOPE) which we can also plot. Here, brighter regions indicate greater uncertainty."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUnJn8dSHPQm"
      },
      "outputs": [],
      "source": [
        "f, s = plt.subplots(2,3,figsize=(24,10))\n",
        "f.suptitle('Uncertainty for each subject')\n",
        "\n",
        "\n",
        "for c,v in enumerate(s.reshape(-1)):\n",
        "  if c==5:\n",
        "    break\n",
        "  varcopes=results_fixed_effects[c][1]\n",
        "  vas_rgb_v=cortex.Vertex(varcopes, subject='hcp_999999', cmap='cubehelix',vmin=0.15,vmax=0.31) \n",
        "  v.title.set_text(\"Subject {sub}\".format(sub=str(c+1)))\n",
        "\n",
        "  cortex.quickshow(vas_rgb_v, with_labels=False, with_rois=False,fig=v);\n",
        "\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDcl5m_yaN3w"
      },
      "source": [
        "And of course, we have the t-statistics themselves. Here I plot negative t statistics (ð›½  (scenes) -  ð›½  (bodies)) in blue and positive t statistics (ð›½  (bodies) -  ð›½  (scenes))  in red. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrap7j67V1h0"
      },
      "outputs": [],
      "source": [
        "f, s = plt.subplots(2,3,figsize=(24,10))\n",
        "f.suptitle('Subject t statistics responses')\n",
        "\n",
        "\n",
        "for c,v in enumerate(s.reshape(-1)):\n",
        "  if c==5:\n",
        "    break\n",
        "  tstat=results_fixed_effects[c][2]\n",
        "  vas_rgb_v=cortex.Vertex(tstat, subject='hcp_999999', cmap='BuWtRd',vmin=-2,vmax=2) \n",
        "  v.title.set_text(\"Subject {sub}\".format(sub=str(c+1)))\n",
        "\n",
        "  cortex.quickshow(vas_rgb_v, with_labels=False, with_rois=False,fig=v);\n",
        "\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYe2ToUlaqWF"
      },
      "source": [
        "Let's threshold the t statistic above 2 so that we can see the regions that have a significantly larger response to bodies v scenes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7w52f8uWfXf"
      },
      "outputs": [],
      "source": [
        "f, s = plt.subplots(2,3,figsize=(24,10))\n",
        "f.suptitle('Tstat > 2')\n",
        "\n",
        "\n",
        "for c,v in enumerate(s.reshape(-1)):\n",
        "  if c==5:\n",
        "    break\n",
        "  tstat=results_fixed_effects[c][2]\n",
        "  tstat[tstat<2]=np.nan\n",
        "  vas_rgb_v=cortex.Vertex(tstat, subject='hcp_999999', cmap='plasma',vmin=2,vmax=5) \n",
        "  v.title.set_text(\"Subject {sub}\".format(sub=str(c+1)))\n",
        "\n",
        "  cortex.quickshow(vas_rgb_v, with_labels=False, with_rois=False,with_curvature=True,fig=v);\n",
        "\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QE_t53aa5wi"
      },
      "source": [
        "There is actually a tremendous amount of consistency here. Allow me to illustrate this by zooming into a portion of lateral visual cortex in the right hemisphere (it's more or less for the left hemisphere).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PvClfYjFbLlT"
      },
      "outputs": [],
      "source": [
        "zoomrect=[70,160,-90,40]\n",
        "\n",
        "def zoom_to_rect(myrect):\n",
        "    plt.axis(myrect)\n",
        "\n",
        "f, s = plt.subplots(2,3,figsize=(15,10))\n",
        "f.suptitle('Tstat > 2 for lateral cortex - right hemisphere')\n",
        "\n",
        "\n",
        "for c,v in enumerate(s.reshape(-1)):\n",
        "  if c==5:\n",
        "    break\n",
        "  tstat=results_fixed_effects[c][2]\n",
        "  tstat[tstat<2]=np.nan\n",
        "  vas_rgb_v=cortex.Vertex(tstat, subject='hcp_999999', cmap='plasma',vmin=2,vmax=5) \n",
        "  v.title.set_text(\"Subject {sub}\".format(sub=str(c+1)))\n",
        "\n",
        "  cortex.quickshow(vas_rgb_v, with_labels=False, with_rois=False,with_curvature=True,fig=v);\n",
        "  zoom_to_rect(zoomrect)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxEERpYLcGdh"
      },
      "source": [
        "\n",
        "\n",
        "You see what I mean about consistency?\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "You should be able to make out 2 different clusters here. The upper right one (upper in this flatmap is more lateral in the brain) is generally larger and more rounded. The lower one (lower in this flatmap is more ventral in the brain) is more elongated and goes in a upper left to lower right diagonal direction.\n",
        "\n",
        "For my money, the former cluster is the extrastriate body area (EBA) and the latter is the fusiform body area (FBA).\n",
        "\n",
        "<img src = 'https://www.frontiersin.org/files/Articles/13143/fnhum-05-00124-r2/image_m/fnhum-05-00124-g001.jpg' width = \"300\" height = \"\" >\n",
        "\n",
        "\n",
        "\n",
        "These are canonical body-selective regions that are known for their selectivity to visual presentations of bodies above and beyond other categories of stimuli. You can read a little background on these regions [in this review paper](https://d1wqtxts1xzle7.cloudfront.net/42308990/Kroppsperception-with-cover-page-v2.pdf?Expires=1669061060&Signature=X29ebbEn-fIMHSwCggAd3IZHW38CHt8Cvyu8rICtOU32zfUYJedBaZlMI05M3yZXIXL58IifwujWuQugkLj-e9Vjj56YOPG6bVO1rYi4Byn2vr7cbTjkFkwRqxxybJJFq6KHG6-vFjOpcuBu4PDOxj4ZMgU~7ZGYaThPEQTgJa36tXDyx~myVpaGiFFou~W7oS-Bs2KT8vQRhEbODec2hs0fcD764iA34i-fC1wL8dWI6Ad8WvmDm-6QTLo0bdbn9zjQfxpPdMuEys5nn0PsxXxXIE-cO-G5H47O13Ewm-iVn7yaGQq4xFt68q8OhSb6X2gxtDjyCh98uubuztRTVg__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA). But trust me, this is what we are looking at. ðŸ˜€ \n",
        "\n",
        "&nbsp;\n",
        "\n",
        "Lets now try and visualise the overlap of these significant regions for each subject. We can do this by:\n",
        "\n",
        "1. Creating a binary mask that is 1 for tstat >2 and 0 for tstat2 <2 for each subject.\n",
        "2. Taking the sum of these masks.\n",
        "3. The resulting image therefore reflects the number of subjects that have tstats >2 at each location."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hmr63-H6cF0x"
      },
      "outputs": [],
      "source": [
        "tstatmasks=np.array([results_fixed_effects[c][2]>2 for c,v in enumerate(results_fixed_effects)])\n",
        "summed_mask=np.sum(tstatmasks,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "elJ7jI4Id8e5"
      },
      "outputs": [],
      "source": [
        "vas_rgb_v=cortex.Vertex(summed_mask, subject='hcp_999999', cmap='gist_ncar',vmin=0,vmax=5) \n",
        "\n",
        "cortex.quickshow(vas_rgb_v, with_labels=False, with_rois=False,with_curvature=True);\n",
        "zoom_to_rect(zoomrect)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5vD8V92fToL"
      },
      "source": [
        "Here, we have a representation of the number of subjects that have a t statistic of >2 at each location. \n",
        "\n",
        "\n",
        "\n",
        "1. Dark blue = 0 subjects\n",
        "2. Light blue/ turquoise = 1 subject\n",
        "3. Green = 2 subjects\n",
        "4. Orange = 3 subjects\n",
        "5. Pink = 4 subjects\n",
        "6. White = 5 subjects\n",
        "\n",
        "Hence, we actually see that a lot of this portion of cortex is significant for the majority of subjects.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q7exh_QmYxJ"
      },
      "source": [
        "## Single subject analyses to group analyses.\n",
        "\n",
        "\n",
        "Critically, what we have looked at so far are a bunch of single subject t-tests. Thus they answer the quite limited question:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "\"**What regions of the brain have significantly larger response to bodies than scenes for this subject?**\" \n",
        "\n",
        "&nbsp;\n",
        "\n",
        "What we really want, is to combine these lower level statistics into one t statistic that answers something more like:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "\"\"**What regions of the brain have significantly larger response to bodies than scenes in the general population?**\"\"\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "In other words, we want to use our sample to make *inferences* about the *population*.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "<img src = 'https://online.stat.psu.edu/public/stat800/lesson04/InferenceGraphicSU17.png' width = \"300\" height = \"\" >\n",
        "\n",
        "\n",
        "This is not something that we can do with a fixed effects model. Fixed effects modeling is legitimate for combining data across runs - however it is ill-suited for making inferences like this. \n",
        "\n",
        "Thus, In order to answer the latter question, we need to model the inter-subject variability in this effect with a *random-effects (RFX)* model. \n",
        "\n",
        "\n",
        "I recomend watch this brief video that goes into the difference between these two types of model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Fptq7ZZQCyNA"
      },
      "outputs": [],
      "source": [
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo('-abMLQSjMSI')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFDa5akCDOoA"
      },
      "source": [
        "&nbsp;\n",
        "\n",
        "<img src = 'https://dragonflytraining.files.wordpress.com/2013/10/man-with-question-01.png?w=600&h=600' width = \"100\" height = \"\" >\n",
        "\n",
        "**QUESTION:** What is your hunch about whether these EBA and FBA activations we looked at will be significant at the Group level? Why?\n",
        "\n",
        "\n",
        "# Take a break 2.\n",
        "\n",
        "Ok, lets take a break and discuss what we have done so far.\n",
        "\n",
        "<img src = 'https://upload.wikimedia.org/wikipedia/commons/8/81/Stop_sign.png' width = \"200\" height = \"\" >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QjzNvjf9Vn4"
      },
      "source": [
        "## Random effects model: Combining estimates across subjects to generalize to the population."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9piYSRkEKn2"
      },
      "source": [
        "Ok, let's think about what we have done so far.\n",
        "\n",
        "We have computed a fixed effects estimate of the single subject contrast ($c\\hat{\\beta}$) ${\\beta}$  (bodies) - ${\\beta}$  (scenes). Therefore we have one of these contrasts for each subject.\n",
        "\n",
        "Let's recap on the GLM equation for a first level model\n",
        "\n",
        "\\begin{align}{y} = X\\beta + e\\end{align} \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ge2erBLFKV8"
      },
      "source": [
        "At the first level\n",
        "\n",
        "1. X is our design matrix.\n",
        "2. Y is our single-subject data.\n",
        "3. ${\\beta}$ estimates the influence that each of our predictors have on the signal.\n",
        "4. *e* is the error of our model - the discrepancy between our model predictions and the data.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "At the second level, the GLM equation is the same, but we use different notation to indicate we are now at the group level.\n",
        "\n",
        "\\begin{align}\n",
        "y^{\\dagger} = \\mathbf{X}^{\\dagger}\\beta^{\\dagger} + \\epsilon^{\\dagger}\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "The results from our run-level analyses ($c\\hat{\\beta}$) across different subjects will become our target ($y^{\\dagger}$). Note that we will use the \"dagger\" ($^{\\dagger}$) superscript to denote that the mathematical terms belong to the group-level model.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "\n",
        "To reiterate, the results from our lower-level analyses ($c\\hat{\\beta}$) become our dependent variable in our group-level analysis ($y^{\\dagger}$):\n",
        "\n",
        "\\begin{align}\n",
        "y^{\\dagger} = c\\hat{\\beta}\n",
        "\\end{align}\n",
        "\n",
        "Here is a graphical representation of this. In this example, we see the individual subject contrast ($c\\hat{\\beta}^{*}$) of ${\\beta}$ faces v ${\\beta}$ houses becoming an element of $Y^{\\dagger}$ for the second level.  \n",
        "\n",
        "\n",
        "<img src = 'https://dartbrains.org/_images/TwoLevelModel.png' width = \"700\" height = \"\" >\n",
        "\n",
        "\n",
        "Again, the group-level represents a GLM with a particular design matrix ($\\mathbf{X}^{\\dagger}$) and parameters ($\\beta^{\\dagger}$):\n",
        "\n",
        "\\begin{align}\n",
        "y^{\\dagger} = \\mathbf{X}^{\\dagger}\\beta^{\\dagger} + \\epsilon^{\\dagger}\n",
        "\\end{align}\n",
        "\n",
        "And the group-level parameters can be estimated with OLS in exactly the same way as at the first level:\n",
        "\n",
        "\\begin{align}\n",
        "\\hat{\\beta}^{\\dagger} = (\\mathbf{X}^{\\dagger\\ T} \\mathbf{X}^{\\dagger})^{-1}\\mathbf{X}^{\\dagger}y^{\\dagger}\n",
        "\\end{align}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsrMz8yQJZYD"
      },
      "source": [
        "Thus, the very first thing that we need to do is construct our ($\\mathbf{Y}^{\\dagger}$)  from the first-level effects ($c\\hat{\\beta}$).\n",
        "\n",
        "Accordingly, below we go through our list of fixed-effects results and bundle our lower $c\\hat{\\beta}$ for each subject into a matrix to create our($\\mathbf{Y}^{\\dagger}$).\n",
        "\n",
        "<img src = 'https://dragonflytraining.files.wordpress.com/2013/10/man-with-question-01.png?w=600&h=600' width = \"100\" height = \"\" >\n",
        "\n",
        "**QUESTION**: Before going ahead, try to guess what shape this matrix will have."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JTKCpZ7W4_Gw"
      },
      "outputs": [],
      "source": [
        "betas_to_y_dagger=np.array([v[0] for c,v in enumerate(results_fixed_effects)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7hg5WfDwKyL4"
      },
      "outputs": [],
      "source": [
        "betas_to_y_dagger.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMVbtZ0GK5Pf"
      },
      "source": [
        "Thus, our ($\\mathbf{Y}^{\\dagger}$) is 5 (subjects) * 118584 (voxels), with each value reflecting the lower level effect ($c\\hat{\\beta}$).\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "Lets think now about our design matrix ($\\mathbf{X}^{\\dagger}$) . As I mentioed, we want to ask the question:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "\"\"**What regions of the brain have significantly larger response to bodies than scenes in the general population?**\"\"\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "\n",
        "Since we have already computed the difference between the body and scene betas at the lower level, this is now a question that is suitable for being adressed via a *one sample t test*. In other words, our null hypothesis is that the overall mean ($c\\hat{\\beta}$) =0.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "Here is a useful table that details some typical design matrices for second-level analyses:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "<img src = 'https://dartbrains.org/_images/DesignMatrices.png' width = \"500\" height = \"\" >\n",
        "\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "Look at the difference in complexity between the design matrix for a one sample and paired sample t-test. As you can see, we kind of dodged a bullet here by computing the ($c\\hat{\\beta}$) at the lower level. \n",
        "\n",
        "\n",
        "If we didnt do this, we would have to to include a whole bunch of extra regressors to model out the participant means ðŸ˜Œ. We have already done this by essentially carrying a difference score ($c\\hat{\\beta}$) up to the second level instead of the ${\\beta}$ for bodies and scenes seperately.\n",
        "\n",
        "As such, as you can see here all we have to do is construct a design matrix with one column, that consists of ones for all rows. This models the mean ($c\\hat{\\beta}$) across subjects. A design matrix for a one sample t-test is as simple as that!\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SXWSaFrJ7o6F"
      },
      "outputs": [],
      "source": [
        "X_dagger=np.ones((5,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RxK6KlkqDAkX"
      },
      "outputs": [],
      "source": [
        "plt.matshow(X_dagger, fignum=1, aspect='auto')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_AA0GEfN58Q"
      },
      "source": [
        "Unlike the first level design matrix, this all appears in the same color because it is just a column of ones. **Simple**\n",
        "\n",
        "Now, we have ($\\mathbf{Y}^{\\dagger}$)  and ($\\mathbf{X}^{\\dagger}$) all we need to do is fit our model to estimate ($\\beta^{\\dagger}$) - our sample mean.\n",
        "\n",
        "The cool thing is, since the GLM equation is the same as it was at the first level, we can actually use exactly the same function that we used at the lower level.\n",
        "\n",
        "The only difference is that we need a different *contrast*.\n",
        "\n",
        "Here, I define this contrast vector as [1]. Since the dot product of the ($\\beta^{\\dagger}$) and 1 will just multiply the only column of ($\\beta^{\\dagger}$) by one - all this does is select out the sample mean that we model. Therefore it barely deserves to be called a contrast, since it is identical to ($\\beta^{\\dagger}$)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aMVnhvgX7gNg"
      },
      "outputs": [],
      "source": [
        "result_random_effects=compute_contrast(X_dagger,betas_to_y_dagger.T,[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Gjb4T2aPLt-"
      },
      "source": [
        "I do hope you are as excited as I am to see the group-level results....\n",
        "\n",
        "Lets take a look at the thresholded t statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HMjvcVQ796RG"
      },
      "outputs": [],
      "source": [
        "tstat_thresh=np.copy(result_random_effects[2])\n",
        "tstat_thresh[tstat_thresh<2]=np.nan\n",
        "vas_rgb_vt=cortex.Vertex(tstat_thresh, subject='hcp_999999', cmap='plasma',vmin=2,vmax=5) \n",
        "\n",
        "cortex.quickshow(vas_rgb_vt, with_labels=False, with_rois=False,with_curvature=True);\n",
        "plt.title(\"Group level results\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvfwdNygI8Dk"
      },
      "source": [
        "It looks as though we have some significant results at the group level!\n",
        "\n",
        "Dont get too excited yet though... things are about to get a bit more complicated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_bzOExgRFFa"
      },
      "source": [
        "If we look at our t numerator we can see the average effect - or sample mean for our body-scene contrast."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ir_DM82pQ88x"
      },
      "outputs": [],
      "source": [
        "copes=result_random_effects[0]\n",
        "vas_rgb_v=cortex.Vertex(copes, subject='hcp_999999', cmap='cubehelix',vmin=np.nanmin(copes),vmax=np.nanmax(copes)) \n",
        "cortex.quickshow(vas_rgb_v, with_labels=False, with_rois=False,with_curvature=True);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-u-L_E-RTwn"
      },
      "source": [
        "If we look at our t denominator, we can see the *between subject variability in effects*. Here, brighter colors indicate regions that have more variable responses across subjects.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "Note that this is different to the fixed effects model, where our denominator reflected the *within-subject uncertainty* in the model's ability to explain the timeseries data, with respect to the *e* of the model and the design variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TH1l4yo9Qg2i"
      },
      "outputs": [],
      "source": [
        "varcopes=result_random_effects[1]\n",
        "vas_rgb_v=cortex.Vertex(varcopes, subject='hcp_999999', cmap='cubehelix',vmin=np.nanmin(varcopes),vmax=np.nanmax(varcopes)) \n",
        "cortex.quickshow(vas_rgb_v, with_labels=False, with_rois=False,with_curvature=True);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlHwQwgFRoi-"
      },
      "source": [
        "Now, what about the EBA and FBA regions we looked at earlier? Are these significant at the group level?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8ersCNO0RpH9"
      },
      "outputs": [],
      "source": [
        "cortex.quickshow(vas_rgb_vt, with_labels=False, with_rois=False,with_curvature=True);\n",
        "zoom_to_rect(zoomrect)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i11ApLieTHYE"
      },
      "source": [
        "Indeed they are.\n",
        "\n",
        "Before we go any further, Id like us to free up some memory by deleting some variables we dont need anymore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YadMJngOS_IF"
      },
      "outputs": [],
      "source": [
        "del ts_data1,ts_data2,run_1_data,run_2_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PCmYhRbR85t"
      },
      "source": [
        "Before wrapping up this section, I would like us to take a look at just one voxel, just to make sure that we know what we are estimating."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uQg5nIgJToBg"
      },
      "outputs": [],
      "source": [
        "vert_number=86820\n",
        "\n",
        "plt.scatter(np.ones(5), betas_to_y_dagger[:,vert_number], s=120, facecolors='none', edgecolors='r')\n",
        "plt.ylabel(\"Single subject effect size\")\n",
        "plt.axhline(y=0,c='k')\n",
        "plt.axhline(y=copes[vert_number],c='r',label='group mean (cope)')\n",
        "plt.errorbar(x=1, y=copes[vert_number], yerr=varcopes[vert_number], markersize=8,fmt='o',label='variance (varcope)',uplims=True, lolims=True)\n",
        "plt.legend()\n",
        "plt.ylim([-0.5,3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ishy6nkMczwr"
      },
      "source": [
        "Here, for one voxel, I have plotted the individual subject estimates of ($c\\hat{\\beta}$) for each subject (red circles). I then plot the sample mean ($\\beta^{\\dagger}$), which is what we estimate at the second level as a red line. I also plot the uncertainty of this effect (blue error bar) - which summarises the between subject uncertainty.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "Thus for every voxel in the brain, we have estimated a sample mean and estimate of between-subject uncertainty like this.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "We can see that this particular voxel has high sample mean (far away from zero, where the null hypothesis is true) and not very much variability between subjects. This is because all of the underlying single subject estimates seem to have quite similar estimates of ($c\\hat{\\beta}$).\n",
        "\n",
        "Recall the formula for the t statistic:\n",
        "\n",
        "\\begin{align}\n",
        "t_{\\hat{\\beta}} = \\frac{\\hat{\\beta}}{\\mathrm{SE}_{\\hat{\\beta}}}\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "Hence, since the t value is the size of the effect scaled by uncertainty, we should expect this voxel to have a high t value.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BbMnFP8v_xXr"
      },
      "outputs": [],
      "source": [
        "tstat_thresh[vert_number]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kX3u2ZQffwu1"
      },
      "source": [
        "Since the t value is high, we also should expect it to have a low p value too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tdVFPkTPfZvV"
      },
      "outputs": [],
      "source": [
        "import scipy.stats as stats\n",
        "t_dist = stats.t(4)\n",
        "p = 1 - t_dist.cdf(tstat_thresh[vert_number])\n",
        "p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQFVZ_eJLPJr"
      },
      "source": [
        "Thus, we can reject the null hypothesis and say:\n",
        "\n",
        "**\"For this voxel, bodies have a significantly larger influence on its response than scenes in the general population.\"**\n",
        "\n",
        "This is exactly the kind of group-level inference we wanted to make. \n",
        "\n",
        "&nbsp;\n",
        "\n",
        "However, unfortunately this gets more complicated......"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCFnBM4uf8gi"
      },
      "source": [
        "# Take a break 3.\n",
        "\n",
        "Ok, lets take a break and discuss what we have done so far.\n",
        "\n",
        "<img src = 'https://upload.wikimedia.org/wikipedia/commons/8/81/Stop_sign.png' width = \"200\" height = \"\" >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2JhB2ZjgFma"
      },
      "source": [
        "## Correction for multiple comparisons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLkmH_3F3miz"
      },
      "source": [
        "So far, we have done *no correction for multiple comparisons*. \n",
        "\n",
        "&nbsp;\n",
        "\n",
        "This problem is explained nicely by the below comic.\n",
        "\n",
        "The comic shows a lab finding a link between acne and jelly beans when a hypothesis was tested at an $\\alpha = 0.05$ significance level. Although there is no link between jelly beans and acne, a significant result was found simply by virtue of the fact that the test was conducted multiple times.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "Since the lab tested 20 colors of jelly beans an $\\alpha = 0.05$ this led to 1 jelly bean being incorrectly fingered as being the acne culprit (20 tests * .05 = 1 false positve). The implications for false discovery in hypothesis testing is that if you repeat a test enough times, youâ€™re going to find an effect â€¦ but that effect may not actually exist.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "<img src = 'https://imgs.xkcd.com/comics/significant.png' width = \"700\" height = \"\" >\n",
        "\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "This is particularly problematic for our data because there is a test being performed for every voxel in our dataset. If we use a threshold p < 0.05 for each individual test, we would expect many voxels to be declared significant even if there were no true effect. In other words, we would make many type I errors.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "\n",
        "To see why, consider a standard $\\alpha = 0.05$.\n",
        "For a single test, our probability of making a type I error is 0.05.\n",
        "The probability of making at least one type I error in\n",
        "$N_{\\mathrm{test}}$ independent tests is then given by\n",
        "$1 - (1 - \\alpha)^{N_{\\mathrm{test}}}$\n",
        "\n",
        "Thus, as show below, even if we conduct just 100 tests, we are virtually guaranteed at least one false positive. The scary thing is that we are performing 118584 tests...."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QTjmJAgL3vPM"
      },
      "outputs": [],
      "source": [
        "N = np.linspace(1, 100,100)\n",
        "alpha = 0.05\n",
        "p_type_I = 1 - (1 - alpha) ** N\n",
        "fig, ax = plt.subplots(figsize=(4, 3))\n",
        "ax.scatter(N, p_type_I, 3)\n",
        "ax.set(xlim=N[[0, -1]], ylim=[0, 1], xlabel=r'$N_{\\mathrm{test}}$',\n",
        "       ylabel=u'Probability of at least\\none type I error')\n",
        "ax.grid(True)\n",
        "fig.tight_layout()\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcN3kd7J5aIR"
      },
      "source": [
        "### Bonferroni correction\n",
        "Perhaps the simplest way to deal with multiple comparisons, the [Bonferroni\n",
        "correction](https://en.wikipedia.org/wiki/Bonferroni_correction)\n",
        "conservatively divides our alpha level by the number of tests. Thus, in our case, we would have to reach the much smaller *p* value of 0.00024 (.05/118584) to reject the null. Let's apply this correction and see which of our voxels values meet this new threshold. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s00CTQNlf70M"
      },
      "outputs": [],
      "source": [
        "pvals = 1 - t_dist.cdf(result_random_effects[2])\n",
        "data_to_plot=cortex.Vertex2D(result_random_effects[2],pvals<(.05/betas_to_y_dagger.shape[1]),vmin=2,vmax=5,vmin2=0,vmax2=1,subject='hcp_999999',cmap='plasma_alpha')\n",
        "flatmap=cortex.quickshow(data_to_plot,with_curvature=True,with_rois=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4wr71b66IFq"
      },
      "source": [
        ".......\n",
        "\n",
        "<img src = 'https://i.pinimg.com/736x/e8/c5/ac/e8c5aceeedf18ea422eb9a7898f9bc72.jpg' width = \"100\" height = \"\" >\n",
        "\n",
        "The reason this plot is blank is that we are left with no voxels surviving this correction. This really highlights how punitive this correction can be.\n",
        "\n",
        "<img src = 'https://dragonflytraining.files.wordpress.com/2013/10/man-with-question-01.png?w=600&h=600' width = \"100\" height = \"\" >\n",
        "\n",
        "**QUESTION:** What do you think is the main reason we are no longer getting any significant results?\n",
        "\n",
        "\n",
        "### FDR correction\n",
        "As you've seen so far, uncorrected results are probably too relaxed and Bonferroni-corrected results are probably too stingent. [The \"False Discovery Rate-correction\" (FDR)](https://en.wikipedia.org/wiki/False_discovery_rate#Classification_of_multiple_hypothesis_tests) technique is a method to adjust $p$-values in a way that falls somewhere between the two approaches.\n",
        "\n",
        "Essentially, while  Bonferroni tries to control the chance of finding at least one false positive result **amongst all your tests**, the FDR-method limits the proportion of false positives **amongst all your tests which turned out significant**. So, if you set your \"FDR-proportion\" (confusingly also referred to as \"alpha\") to 0.05, then it will adjust your initial $p$-values such that out of all your significant results, on average 5% will be false positives. You can read more about the simple mathematics of the FDR correction [here](https://wiki.q-researchsoftware.com/wiki/False_Discovery_Rate_Correction) and there is amore detailed, graphical summary[here](https://matthew-brett.github.io/teaching/fdr.html) \n",
        "\n",
        "Let's check out what our results look like after FDR correction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NO1v8Ssiyhm"
      },
      "outputs": [],
      "source": [
        "from statsmodels.stats.multitest import fdrcorrection\n",
        "alpha_fdr = 0.05\n",
        "\n",
        "fdr_result = fdrcorrection(np.nan_to_num(pvals,nan=np.nanmean(pvals)), alpha=alpha_fdr)\n",
        "\n",
        "\n",
        "data_to_plot=cortex.Vertex2D(result_random_effects[2],fdr_result[0],vmin=2,vmax=5,vmin2=0,vmax2=1,subject='hcp_999999',cmap='plasma_alpha')\n",
        "flatmap=cortex.quickshow(data_to_plot,with_curvature=True,with_rois=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMl6nCjdDOAh"
      },
      "source": [
        "\n",
        "<img src = 'https://i.pinimg.com/736x/e8/c5/ac/e8c5aceeedf18ea422eb9a7898f9bc72.jpg' width = \"100\" height = \"\" >\n",
        "\n",
        "We are left with no voxels surviving this correction either.\n",
        "\n",
        "\n",
        "We can also visualise the corrections that we have made by plotting our corrected p values for each method against the original p value.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgX9lDXD5Jey"
      },
      "outputs": [],
      "source": [
        "bonf_corrections=pvals*betas_to_y_dagger.shape[1]\n",
        "bonf_corrections[bonf_corrections>=1]=1\n",
        "\n",
        "plt.plot(fdr_result[1],pvals,'*',label='FDR correction')\n",
        "plt.plot(bonf_corrections,pvals,'*',label='bonferroni correction')\n",
        "plt.xlabel(\"FDR p value\")\n",
        "plt.ylabel(\"Orignal p value\")\n",
        "plt.axvline(x=0.05,c='k')\n",
        "plt.axhline(y=0.05,c='k')\n",
        "\n",
        "plt.text(x=0.2, y=.7,s='min bonf p value = {bonf}'.format(bonf=np.nanmin(bonf_corrections)))\n",
        "plt.text(x=0.2, y=.6,s='min FDR p value = {FDR}'.format(FDR=np.nanmin(fdr_result[1])))\n",
        "\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ0vC3clODN4"
      },
      "source": [
        "Here we see that a lot of our original p values were significant (values below black horizontal line), but none of our corrected p values are significant (values to the left of the vertical line)\n",
        "\n",
        "\n",
        "We can see that we get a lot closer to a p value of .05 (black vertical line) with the FDR method. (blue) than with the bonferroni method (orange). In fact, the vast majority of our bonferroni-adjusted p values are 1 (they cannot exceed 1 since this is a probability) - which really highlights how stringent this procedure is."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L02SKOZ0PdGw"
      },
      "source": [
        "### Cluster-based correction.\n",
        "\n",
        "Ok, let's think about this a little differently. Let's take a look at our raw tstats.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2-tIa4NMqsI"
      },
      "outputs": [],
      "source": [
        "vas_rgb_vt=cortex.Vertex(result_random_effects[2], subject='hcp_999999', cmap='BuWtRd',vmin=-2,vmax=2) \n",
        "\n",
        "cortex.quickshow(vas_rgb_vt, with_labels=False, with_rois=False,with_curvature=True);\n",
        "plt.title(\"Group level results\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5uxhYJrQGj5"
      },
      "source": [
        "The assumption we have been making so far is that our tests are independent of one another. \n",
        "\n",
        "However, do these tests really look independent of one another? We see that high t statistics are adjacent with high t statistics (red) and low t statistics are generally adjacent with low t statistics (blue).\n",
        "\n",
        "If our data were truly independent, we would see something that looked more like this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3xX5v5VEPIa"
      },
      "outputs": [],
      "source": [
        "base_ts=np.copy(result_random_effects[2])\n",
        "rand_ts=np.random.shuffle(base_ts)\n",
        "\n",
        "vas_rgb_vt=cortex.Vertex(base_ts, subject='hcp_999999', cmap='BuWtRd',vmin=-2,vmax=2) \n",
        "\n",
        "cortex.quickshow(vas_rgb_vt, with_labels=False, with_rois=False,with_curvature=True);\n",
        "plt.title(\"Group level results\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaGq83GwRl4J"
      },
      "source": [
        "Something that we know about fMRI activations is that they are spatially correlated. We tend to see 'clusters' of activations rather than really dotty t statistics like this. In other words, if we know that a certain voxel is signficant in a certain test, it is quite likely that the voxels directly next to it are also significant. The same applies to non-significant voxels. \n",
        "\n",
        "&nbsp;\n",
        "\n",
        "Hence, we are not really conducting 118584 independent tests - but some number lower than that that is related to number of these clusters. Thus, each of the aforementioned multiple comparisons corrections have the disadvantage of not fully incorporating the correlation structure of the data, namely that points close to one another tend to be correlated. However, by defining the adjacency (or â€œneighborâ€) structure in our data, we can use a **cluster-based** approach to compensate.\n",
        "\n",
        "Cluster based approaches all start with defining **clusters**, which are contiguous (connecting) regions of the brain that all exceed a cetain statistic.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "<img src = 'https://i.imgur.com/xZfkKoH.png' width = \"500\" height = \"\" >\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "Here we see the typical effect of different levels of thresholding on \n",
        "cluster definitions ; the resulting clusters are randomly color-coded to show\n",
        "which voxels belong to each cluster. At the lowest threshold, there is one large cluster that\n",
        "encompasses much of the brain, whereas higher thresholds break up this cluster, at the\n",
        "expense of excluding many regions that do not survive the higher threshold.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "Given that we know that clusters are a meaningful unit in fMRI analysis, perhaps we can reframe our orginal question:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "\"what was the chance of finding a t value this large for the voxel if the null hypothesis was true?\"\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "And instead, we can ask a different question, which is:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "*\"What was the chance of finding a **cluster of this size** if the null hypothesis was true?\"*\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "This thus allows us to take into account not just the raw t-statistics, **but the sum of t statistics within a cluster**. In doing this,we don't really base everything soley on raw t-statistics, but more about the *spatial extent* of these statistics - thereby respecting the correlation structure of the data.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "By extentsion, we effectively penalize small clusters with few connecting voxels (these tend to be noise anyway), and reward clusters that are larger. \n",
        "\n",
        "\n",
        "There are in fact a number of ways of performing these cluster based corrections. I am going to focus on just one of these."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YU16CBTHTuRm"
      },
      "source": [
        "\n",
        "### Permutation-based cluster thresholding\n",
        "\n",
        "In permutation-based cluster thresholding, we we leverage the [principle of exchangeability](https://www.ohbmbrainmappingblog.com/blog/a-brief-overview-of-permutation-testing-with-examples).\n",
        "\n",
        "In other words, if the null hypothesis is true, the sign of our ($c\\hat{\\beta}$) (positive or negative) becomes arbitrary - and we can flip it without changing the distribution of the test statistic. \n",
        "\n",
        "Therefore, we can construct a null-distribution of statistics by taking random permutation of subjects, flipping the sign of the ($c\\hat{\\beta}$), and recording the value of some statistic we compute from this permuted datset.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "Thus, in this case, for each of these random permutation, we would:\n",
        "\n",
        "1. Compute the t value for each voxel individually.\n",
        "\n",
        "2. Threshold the t values to define significant ones.\n",
        "\n",
        "3. Define clusters (adjacent voxels that exceed this threshold).\n",
        "\n",
        "4. Retain the the maximum sum of t-values within a cluster to build the null distribution.\n",
        "\n",
        "\n",
        "The p-value is simply the proportion of this null distribution of summed t values that are smaller than those we find in the clusters in the observed data itself.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "\n",
        "**Do not break your head on this too much**. All you really need to know about this procedure is that it evaluates the probability of obtaining a cluster of a given size under randomized conditions that model the null hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpazrhU0bOLI"
      },
      "source": [
        "Ok, lets do this now. I'm only going to do this for one hemisphere to save time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYoyax4ki47J"
      },
      "outputs": [],
      "source": [
        "surfaces=[cortex.polyutils.Surface(*d)\n",
        "                         for d in cortex.db.get_surf('hcp_999999', 'fiducial')]\n",
        "\n",
        "from mne.stats import permutation_cluster_1samp_test\n",
        "\n",
        "t_clust, clusters, p_values, H0 = permutation_cluster_1samp_test(X=betas_to_y_dagger[:,:59292], n_jobs=None, threshold=2.132, adjacency=surfaces[0].adj, out_type='mask',seed=1234)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2s2OmObcDIC"
      },
      "source": [
        "Note that the first thing we notice printed here is that there are 1403 clusters in our data. In other words, there are 1403 regions with at least 2 significant voxels adjoining one another."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMK818JUdDQj"
      },
      "source": [
        "Here, we see the maximum sum of t statistics that we get for each random permutation. This is essentially our 'null distribution'. In order for us to reject the null hypothesis, we should have clusters in our observed data that mostly exceed these values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeH-ObKwce4v"
      },
      "outputs": [],
      "source": [
        "plt.hist(H0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ1MtUATdzRY"
      },
      "source": [
        "So based on this null distribution, what kind of p value do we get for each one of our clusters? Let's see and sort them by lowest first. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eg5T-tS6cxyd"
      },
      "outputs": [],
      "source": [
        "p_values[np.argsort(p_values)][:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6VjHgPZegN1"
      },
      "source": [
        "Oof, we have a few that are very close to .05.\n",
        "\n",
        "Lets take a look at one cluster with a p value of .0625 now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3tdrYQmefJA"
      },
      "outputs": [],
      "source": [
        "data_to_plot=cortex.Vertex2D(result_random_effects[2],clusters[np.argsort(p_values)[0]],vmin=2,vmax=5,vmin2=0,vmax2=1,subject='hcp_999999',cmap='plasma_alpha')\n",
        "flatmap=cortex.quickshow(data_to_plot,with_curvature=True,with_rois=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqeJDmgajGLO"
      },
      "source": [
        "This is a pretty big custer, and we get a pretty big sum of t statistics within it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfVM4IXFV6Ur"
      },
      "outputs": [],
      "source": [
        "np.sum(t_clust[clusters[np.argsort(p_values)[0]]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsJRCpK-jUQk"
      },
      "source": [
        "The only problem is that we actually got a sum of t statistics larger than this for 1/16 of our permuted samples (red dot on the left is slightly higher than the blue line)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lej88paR3UNV"
      },
      "outputs": [],
      "source": [
        "plt.plot(H0,'r*',label='permuted stat')\n",
        "plt.ylabel(\"Max summed t statistic for a cluster\")\n",
        "plt.xlabel(\"Permutation number\")\n",
        "\n",
        "plt.axhline(y=10590.664518759813,linewidth=1, color='b',label='observed value for cluster')\n",
        "\n",
        "plt.ylim([0,11000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4v2jQtmujghq"
      },
      "source": [
        "And 1/6 equals our p value of 0.0625. The chances of finding a cluster of this size under the null hypothesis are still > .05 Hence, we still technically have not found a significant effect...\n",
        "\n",
        "What next?\n",
        "\n",
        "<img src = 'https://dragonflytraining.files.wordpress.com/2013/10/man-with-question-01.png?w=600&h=600' width = \"100\" height = \"\" >\n",
        "\n",
        "**QUESTION:** What possible problems can you see with cluster-based corrections?\n",
        "\n",
        "\n",
        "\n",
        "# Take a break 4.\n",
        "\n",
        "Ok, lets take a break and discuss what we have done so far.\n",
        "\n",
        "<img src = 'https://upload.wikimedia.org/wikipedia/commons/8/81/Stop_sign.png' width = \"200\" height = \"\" >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhYL87_3XoRK"
      },
      "source": [
        "# Region of interest analyses.\n",
        "\n",
        "By now you may start to suspect that conducting an experiment with 5 participants was doomed to fail with respect to detecting a significant effect.\n",
        "\n",
        "However, this might be because we are asking a very unspecific, untargetted question about all of the voxels in the brain - which requires us to correct for the number of tests we are performing. This is punitive to the extent that we cannot detect any effects at the group level.\n",
        "\n",
        "One alternative way to deal with the multiple comparisons problem is a more targetted, confirmatory approach: region-of-interest (ROI) analysis.\n",
        "\n",
        "## What is an ROI?\n",
        "\n",
        "An ROI is a pre-defined are of the brain that is thought to be relevant to our effects under study. These can be anatomically or functionally defined. *Anatomical ROIs* are regions of interest based on anatomical landmarks (such as folding pattern, myelination etc), this might include the caudate nucleus, Heschl's gyrus, and anterior cingulate cortex.\n",
        "\n",
        "Other ROIs are defined based on *functional* properties. Sometimes, the data used for defining such a functional ROI comes from a different run of the same experiment, consisting of a â€œfunctional localizerâ€ to define an ROI for focusing the analysis of other data. For example, [floc](https://github.com/VPNL/fLoc) is a procedure for localising face-selective regions of the brain such as the fusiform area (FFA) and occipital face area (OFA) based on contrasting each voxels activity to faces versus other categories of visual stimuli (words, houses etc) and thresholding the resulting statistics. \n",
        "\n",
        "&nbsp;\n",
        "\n",
        "<img src = 'https://ars.els-cdn.com/content/image/3-s2.0-B9780123970251001585-f00158-01-9780123970251.jpg' width = \"500\" height = \"\" >\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "A slightly different example is that the boundary between the primary visual cortex (V1) and secondary visual cortex (V2) is defined by its selectivity for different parts of the visual field - which can be revealed via a technique called [population receptive field modeling](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3073038/).\n",
        "\n",
        "The color wheel on the bottom right of the below figure represents the portion of the visual field that each portion of visual cortex is tuned to (expressed as polar angle). We can see that V1 smoothly represents a quadrant of the visual field (blue to red & blue to green - going from horizontal to vertical meridians) - but this reverses at the boundary of V2 (red to blue & green to blue going from vertical to horizontal meridians). This reversal in the visual field representation indicates the boundary between two distinct maps of the visual field - allowing us to parse out V1 and V2.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "<img src = 'https://www.researchgate.net/publication/239463768/figure/fig5/AS:667778742771716@1536222267296/Comparison-of-the-polar-angle-retinotopy-in-human-visual-cortex-relative-to-that.png' width = \"500\" height = \"\" >\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "In addition, there are many standardizes *atlases* or *parcellations* of the brain that exist from whcih we can define ROIs. Some of these are summarised [here](https://www.lead-dbs.org/helpsupport/knowledge-base/atlasesresources/cortical-atlas-parcellations-mni-space/). \n",
        "\n",
        "&nbsp;\n",
        "\n",
        "One of the most extensive of these is the [Glasser 2016](https://www.nature.com/articles/nature18933) multimodal parcellation of over 180 cortical regions. \n",
        "\n",
        "&nbsp;\n",
        "\n",
        "<img src = 'https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnature18933/MediaObjects/41586_2016_Article_BFnature18933_Fig3_HTML.jpg?as=webp' width = \"500\" height = \"\" >\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "This is a truly mind-boggling atlas. It leveraged data from task-based functional contrasts, anatomical cues (e.g. cortical folding patterns, thickness) and cytoarchitecture (myelin etc) to define these regions (hence the word 'multimodal'). You can interact with this atlas using this [nice tool](http://corticalexplorer.com/). Take some time to play around with this.\n",
        "\n",
        "I have downloaded this atlas, so we also have this at our disposal here: \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DE12tWoejhj"
      },
      "outputs": [],
      "source": [
        "mm=MMP_masker('/content/data/HCP-MMP')\n",
        "atlas=np.concatenate([mm.lh_labels,mm.rh_labels])\n",
        "\n",
        "vas_rgb_v=cortex.Vertex(atlas, subject='hcp_999999', cmap='gist_ncar',vmin=np.nanmin(atlas),vmax=np.nanmax(atlas)) \n",
        "cortex.quickshow(vas_rgb_v, with_labels=False, with_rois=False,with_curvature=False);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRicI9Ns4E-r"
      },
      "source": [
        "Here, each color represents a different region of the brain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0egJddAmiRaA"
      },
      "source": [
        "We can make 'masks' based on regions defined in this atlas. Here for instance, the fusiform face area (here referred to as the FFC - fusiform face complex is shown in white.\n",
        "\n",
        "We can use masks like this to limit the scope of our analyses - restricting our analyses to voxels within the mask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpT-38TIjNTw"
      },
      "outputs": [],
      "source": [
        "FFC_mask=mm.make_composite_mask(['FFC'])\n",
        "\n",
        "vas_rgb_v=cortex.Vertex(FFC_mask[-1], subject='hcp_999999', cmap='gist_ncar',vmin=np.nanmin(FFC_mask[-1]),vmax=np.nanmax(FFC_mask[-1])) \n",
        "cortex.quickshow(vas_rgb_v, with_labels=False, with_rois=False,with_curvature=False);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esWpoJFi4Sga"
      },
      "source": [
        "## Choosing an ROI.\n",
        "\n",
        "To define an ROI in which to perform a targetted analysis, we would usually consult previous literature to identify structures that could be plausibly related to the effect we are interested in.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "To this end, one tool I find valuable is [neurosynth](https://neurosynth.org/analyses/terms/) Neurosynth is a tool that, amongst other things, can perform â€œautomaticâ€ meta-analyses of fMRI data. As stated on its website, â€œit takes thousands of published articles reporting the results of fMRI studies, chews on them for a bit, and then spits out imagesâ€.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "Take a look at this result [here](https://neurosynth.org/analyses/terms/body/) where I enter the search term 'body'. It reveals z statistics of an â€œassociation testâ€ of your selected term across voxels in MNI space, which tests whether voxels are preferentially related to your selected term. In other words, here we have performed a sort of meta-analytic localiser on regions of the brain that are selective to bodies.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "If we navigate to the peak z statistic (-50,-76,-4), we can then click *'whats here'* and then on the *'associations'* tab to see what other search terms load heavily onto this location.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "<img src = 'https://i.imgur.com/1K16Bbu.png' width = \"200\" height = \"\" >\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "<img src = 'https://i.imgur.com/ch7W1c8.png' width = \"600\" height = \"\" >\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "We see lots of interesting things here. One is 'action observation'- which makes sense given that when we view bodies we usually view them performing actions. Furthermore, we also see labels for different brain regions. One of these is 'V5' and another is 'MT'. Actually, these labels are often [used interchangeably](https://pubmed.ncbi.nlm.nih.gov/15702885/) to refer to the human motion-sensitive visual cortex - which is also why we see the term 'motion' in here.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "\n",
        "Thus, we have good reason to suspect, based on previous literature that area MT will be selective to bodies v scenes. Hence, we can now re-pitch our research question:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "\"*Is there a greater average response to bodies than scenes in human area MT?*\". \n",
        "\n",
        "&nbsp;\n",
        "\n",
        "On the one hand this seems like a bit of a crafty thing to do - side-stepping the multiple comparisons problem like this. However - think of it this way: We can be fairly certain that only a small portion of cortex is going to show body-selectivity. Why then, would we punish ourselves by fishing around the entire brain and suffering all the penalties that we incur for doing this?\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "For instance, earlier on we looked at what we can be fairly certain are the EBA and the FBA. Given the consistency of these regions across subjects and the scope of our question - which is about selectivity to bodies - not a comprehensive model of brain-wide function - its kind of wild to hamstring ourselves with the whole-brain multiple-comparisons problem when we have a good reason for being more specific.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "One interesting factoid for you. It turns out that what we call 'MT' and 'EBA' heavily overlap with one another - to the extent that different groups of researchers are probably using these labels to refer to the same region of the brain.\n",
        "\n",
        "<img src = 'https://i.imgur.com/uXZrkqV.png' width = \"400\" height = \"\" >\n",
        "\n",
        "The region drawn in white here is the EBA, which I defined based on an independent functional contrast between bodies anod other categories of stimulus, using the floc localizer. In pink you can see the location of MT, as estimated by the Glasser parcelation. This just goes to show that these labels we construct for different brain regions are heavily dependent on the data we use to define them:\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "\"*It shows a large response to bodies? Let's call it the body area!*\"\n",
        "\n",
        "\"*It shows a large response to visual motion patterns? Let's call it motion sensitive cortex!*\".\n",
        "\n",
        "&nbsp;\n",
        "\n",
        " It also illustrates that mapping one function to one brain area is probably not a good idea.......\n",
        "\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "Regardless, luckily for us, as you can see [here](https://docs.google.com/spreadsheets/d/1hZYPbEGX_MmsaajTdOcbiI8DxghrtZCT/edit?usp=sharing&ouid=115381703141154937331&rtpof=true&sd=true) area 'MT' (row 25) is a region that is defined in our Glasser multimodal atlas. Therefore, we have everything we need to perform an ROI analysis on MT.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMuVjWQK-wkT"
      },
      "source": [
        "## Setting up $y^{\\dagger}$ for roi analysis.\n",
        "\n",
        "The first thing we want to is limit our $c\\hat{\\beta}$ to those within our ROI, to this end we define a mask.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyuglg844RbO"
      },
      "outputs": [],
      "source": [
        "MT_mask=mm.make_composite_mask(['MT'])\n",
        "\n",
        "vas_rgb_v=cortex.Vertex(MT_mask[-1], subject='hcp_999999', cmap='gist_ncar',vmin=np.nanmin(MT_mask[-1]),vmax=np.nanmax(MT_mask[-1])) \n",
        "cortex.quickshow(vas_rgb_v, with_labels=False, with_rois=False,with_curvature=False);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoe6KpatAH7L"
      },
      "source": [
        "As you can see - we see got some consistently high voxel-wise t statistics within this region. This does seem like an ROI that is a good candidate for detecting a significant effect."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nz46IBGrAAJO"
      },
      "outputs": [],
      "source": [
        "data_to_plot=cortex.Vertex2D(result_random_effects[2],MT_mask[-1],vmin=2,vmax=5,vmin2=0,vmax2=1,subject='hcp_999999',cmap='plasma_alpha')\n",
        "flatmap=cortex.quickshow(data_to_plot,with_curvature=True,with_rois=False)\n",
        "\n",
        "zoomrect=[-140,140,-30,40]\n",
        "\n",
        "zoom_to_rect(zoomrect)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enTB-KGG-v2P"
      },
      "source": [
        "We then take the mean ($c\\hat{\\beta}$) across voxels in this mask for each subject. This gives us 5 values, each reflecting the mean ($c\\hat{\\beta}$) within MT for each subject"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_i7hxN7NprqK"
      },
      "outputs": [],
      "source": [
        "Y_dagger_MT=np.mean(betas_to_y_dagger[:,FFC_mask[-1]],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URyZ8_GMCBef"
      },
      "outputs": [],
      "source": [
        "Y_dagger_MT.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWDk8jZcCC4L"
      },
      "source": [
        "## Fitting the model\n",
        "\n",
        "The model fitting is no different. We fit the random effects model with these voxel-averaged MT ($c\\hat{\\beta}$) as our $y^{\\dagger}$ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HH2Dng_9qA50"
      },
      "outputs": [],
      "source": [
        "result_random_effects_MT=compute_contrast(X_dagger,Y_dagger_MT.T,[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xYmTzG7CdBl"
      },
      "source": [
        "Here, we get a massive t value and p value well less than .05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHDbmRGDB6x3"
      },
      "outputs": [],
      "source": [
        "result_random_effects_MT[2],1 - t_dist.cdf(result_random_effects_MT[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwIk29i2C4iV"
      },
      "outputs": [],
      "source": [
        "plt.scatter(np.ones(5), Y_dagger_MT, s=120, facecolors='none', edgecolors='r')\n",
        "plt.ylabel(\"Single subject effect size for MT\")\n",
        "plt.axhline(y=0,c='k')\n",
        "plt.axhline(y=result_random_effects_MT[0],c='r',label='group mean (cope)')\n",
        "plt.errorbar(x=1, y=result_random_effects_MT[0], yerr=result_random_effects_MT[1], markersize=8,fmt='o',label='variance (varcope)',uplims=True, lolims=True)\n",
        "plt.legend()\n",
        "plt.ylim([-0.25,1.75])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw2293eUD2HY"
      },
      "source": [
        "Since we have asked this very specific, targetted question in a confirmatory way we no longer suffer from the multiple comparisons problem. We can therefore reject our null hypotheses and say \"Images of bodies have more influence on the average response of MT than scenes in the general population\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wlb8mzprE43O"
      },
      "source": [
        "# Summary\n",
        "## Great work!!!\n",
        "\n",
        "We covered a lot of ground here! ðŸ‘\n",
        "\n",
        "1. Fixed effects modeling to combine runs of data.\n",
        "2. Random effects modeling to make inferences about the general population.\n",
        "3. Different methods for dealing with the multiple comparisons problem.\n",
        "4. ROI-based analyses - leveraging existing atlases and online meta-analytic tools.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "---\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "### Some key points\n",
        "\n",
        "1. Second-level analysis operates via the same principles as first-level analysis. We use exactly the same formulae and estimate the parameters in exactly the same way. The only differece is that the variable we are now trying to explain is the parameters estimated at the single-subject level.\n",
        "2. There are multiple methods for correcting for the multiple comparisons problem - all of these have various strengths, weakness and complexities.\n",
        "3. There is a great strength in specific, targetted hypotheses - which keep us from a voxel-wise \"fishing expedition\" and all the problems that incurs. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-Q1hoMEGvwI"
      },
      "source": [
        "# Things to think about\n",
        "\n",
        "1. Would you expect our p values to survive corrections if we had double the participants?\n",
        "2. What effect do you think spatial smoothing might have on our cluster analyses?\n",
        "3. Take a browse of neurosynth and search for 'visual word'. Think of some interesting ROI-based analyses we could perform based on the design matrix we have for our movie. \n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}